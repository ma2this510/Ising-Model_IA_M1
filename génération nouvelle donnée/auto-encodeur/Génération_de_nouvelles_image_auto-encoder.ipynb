{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LrItJGnDdEI"
   },
   "source": [
    "## Importation des libraries et du Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DSR-cp9AtoTh"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, ReLU, UpSampling2D, CategoryEncoding, Reshape, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "p8PSMrAzou5p"
   },
   "outputs": [],
   "source": [
    "file_dir = \"D:/ising/data/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHY2yaDPDsiB"
   },
   "source": [
    "## chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SuaDA9Enmvfp"
   },
   "outputs": [],
   "source": [
    "def read_t(t,root=\"./\", str_check=False):\n",
    "  if not str_check :\n",
    "    data = pickle.load(open(root+'Ising2DFM_reSample_L40_T=%.2f.pkl'%t,'rb'))\n",
    "  else :\n",
    "    data = pickle.load(open(root+f'Ising2DFM_reSample_L40_T={t}.pkl','rb'))\n",
    "  return np.unpackbits(data).astype(int).reshape(-1,1600)\n",
    "\n",
    "\n",
    "\n",
    "Data_All = []\n",
    "Data_Label = []\n",
    "for T in np.arange(0.25,4.25,0.25):\n",
    "    data_all = read_t('%.2f'%T, file_dir, True)\n",
    "    data_all.shape\n",
    "    \n",
    "    mean_val = data_all.reshape(data_all.shape[0], 40*40).mean(axis=1)\n",
    "    data_all = data_all.astype(np.uint8)\n",
    "    \n",
    "    data_all[mean_val > 0.5] = np.invert(data_all[mean_val > 0.5])//255\n",
    "    data_all.shape\n",
    "    data_label = np.array([T]*data_all.shape[0])\n",
    "    Data_All.append(data_all)\n",
    "    Data_Label.append(data_label)\n",
    "    \n",
    "Data_All = np.array(Data_All)\n",
    "Data_Label = np.array(Data_Label)\n",
    "\n",
    "plt.imshow(Data_All[10][0].reshape(40,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(160000), Data_All.reshape(160000, 40*40).mean(axis=1), s=0.05,c = Data_Label.reshape(160000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Suz-ZR-ctgzP"
   },
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cwaWWcEAerXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération data pour toute les températures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEC=[]\n",
    "VEC1=[]\n",
    "Tliste = np.arange(0.25,4.25,0.25)\n",
    "for i in np.flip(np.arange(16)):\n",
    "\n",
    "    time.sleep(1)\n",
    "     \n",
    "    \n",
    "    class Autoencoder(Model):\n",
    "        def __init__(self):\n",
    "            super(Autoencoder, self).__init__()\n",
    "            self.encoder = tf.keras.Sequential([\n",
    "                \n",
    "                layers.Input(shape=(40, 40, 1)),\n",
    "                layers.Conv2D(5, (7, 7), activation='relu', padding='same'),\n",
    "                layers.Conv2D(5, (3, 3),padding='same', activation='relu'),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Conv2D(5, (3, 3),padding='same', activation='relu'),\n",
    "                layers.Conv2D(15, (3, 3), activation='relu', padding='same'),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Conv2D(15, (3, 3), activation='relu', padding='same'),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Conv2D(15, (3, 3), activation='relu', padding='same'),\n",
    "                layers.Conv2D(1, (5, 5), activation='relu', padding='same'),\n",
    "    \n",
    "            ])\n",
    "            \n",
    "            self.decoder = tf.keras.Sequential([\n",
    "                layers.Input(shape=(5, 5, 1)),\n",
    "                layers.UpSampling2D((2,2), interpolation = 'nearest'),\n",
    "                layers.Conv2D(5, (5, 5),padding='same', activation='relu'),\n",
    "                layers.UpSampling2D((2,2), interpolation = 'nearest'),\n",
    "                layers.Conv2D(3, (3, 3),padding='same', activation='relu'),\n",
    "                layers.UpSampling2D((2,2), interpolation = 'nearest'),\n",
    "                layers.Conv2D(3, (5, 5),padding='same', activation='relu'),\n",
    "                layers.Conv2D(1, (3, 3),padding='same', activation='sigmoid'),\n",
    "    \n",
    "            ])\n",
    "        def call(self, x):\n",
    "            encoded = self.encoder(x)\n",
    "            decoded = self.decoder(encoded)\n",
    "            return decoded\n",
    "    \n",
    "    autoencoder = Autoencoder()\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    autoencoder.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    tot_data_cnn = Data_All[i].reshape(Data_All[i].shape[0], 40, 40, 1)\n",
    "    print(tot_data_cnn.shape)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(tot_data_cnn, data_label, test_size=0.2, random_state=42)\n",
    "    history = autoencoder.fit(X_train, X_train, epochs=130, batch_size=264,\n",
    "                  validation_split=0.2)\n",
    "    encoded_imgs = autoencoder.encoder(X_test).numpy()\n",
    "    decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss Evolution T = {}'.format(Tliste[i]))\n",
    "    plt.savefig('D:/ising/evoltution T = {} .jpg'.format(Tliste[i]),dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    n = 10\n",
    "    T = Tliste[i]\n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for j in range(n):\n",
    "        img = np.copy(decoded_imgs[j])\n",
    "        # display original\n",
    "        ax = plt.subplot(3, n, j + 1)\n",
    "        plt.imshow(X_test[j])\n",
    "        plt.title(\"ratio = {}\".format(np.sum(X_test[j].reshape(1600))/1600))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(3, n, j + 1 + n)\n",
    "        plt.imshow(decoded_imgs[j])\n",
    "        plt.title(\"reconstructed\")\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "        ax = plt.subplot(3, n, j + 1 + 2*n)\n",
    "        img= 2*np.random.normal(0.2,0.15,1600) + 1.5*img.reshape(1600) + 6*np.random.normal(-0.1,0.02,1600)\n",
    "        ratio = T/4\n",
    "        sig = -1/(2+2*np.exp(-(T-2)*4))+1\n",
    "        img[img<=sig]=0\n",
    "        img[img>sig]=1\n",
    "        img = img.reshape(40,40)\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"ratio = {}\".format(np.sum(img.reshape(1600))/1600))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.text(-400,60,'image original, prédite avec un espace latent dim = 25, et reconstruite avec bruit, T = {}'.format(Tliste[i]),size = 20)\n",
    "    plt.savefig('D:/ising/triple_T = {} .jpg'.format(Tliste[i]),dpi=300)\n",
    "    plt.show()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    encoded_imgs1 = autoencoder.encoder(X_train).numpy()\n",
    "    vec1 = np.sum(encoded_imgs1.reshape(np.shape(encoded_imgs1)[0],25),axis=0)\n",
    "    VEC1.append(vec1)\n",
    "    vec = np.sum(encoded_imgs.reshape(np.shape(encoded_imgs)[0],25),axis=0)\n",
    "    VEC.append(vec)\n",
    "    if np.sum(vec1)==0:\n",
    "        plt.plot(range(25),vec1,label='X_train')\n",
    "    if np.sum(vec)==0:\n",
    "        plt.plot(range(25),vec,label='X_test')\n",
    "    elif np.sum(vec1)!=0:  \n",
    "        plt.plot(range(25),vec1/np.sum(vec1),label='X_train')\n",
    "    elif np.sum(vec)!=0:  \n",
    "        plt.plot(range(25),vec/np.sum(vec),label='X_test')\n",
    "    plt.xlabel('composante nbr')\n",
    "    plt.ylabel('amplitude')\n",
    "    plt.legend()\n",
    "    plt.title('vecteur moyen des données T = {} dans le sous espace latent'.format(Tliste[i]))\n",
    "    plt.savefig('D:/ising/latent_T = {} .jpg'.format(Tliste[i]),dpi=300)\n",
    "    plt.show()\n",
    "    del X_train\n",
    "    del X_test\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Zh-CYYk3ERYj",
    "2uQPjFf4wPNY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
